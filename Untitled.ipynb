{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finding Lane Lines on the Road** \n",
    "\n",
    "---\n",
    "\n",
    "**Goals**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* Make a pipeline that finds lane lines on the road\n",
    "* Reflect on your work in a written report\n",
    "\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./examples/grayscale.jpg \"Grayscale\"\n",
    "\n",
    "[original]: ./all_images.jpg \"Test images\"\n",
    "[blur]: ./blur.jpg \"Blured images\"\n",
    "[color_select]: ./color_select.jpg \"Color select images\"\n",
    "[edge]: ./edge.jpg \"Edge detected images\"\n",
    "[hough]: ./hough.jpg \"Hough lines images\"\n",
    "[hsl_select]: ./hsl_select.jpg \"HSL color select images\"\n",
    "[lane_drawn]: ./lane_drawn.jpg \"Lane drawn images\"\n",
    "[overlay]: ./overlay.jpg \"Overlay images\"\n",
    "[rgb_select]: ./rgb_select.jpg \"RGB color select images\"\n",
    "[roi]: ./roi.jpg \"ROI images\"\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This is a project to find lane lines on the road with python and opencv. The agenda is to first develop a pipeline which will annotate the lane lines on an image and later extend the technique to find lane line on video streams. This project explores many opencv functions to do image processing together with numpy to accomplish the task of lane detection.\n",
    "\n",
    "### The Concept\n",
    "\n",
    "Lane detection is a complex process. As in most computer vision problems the first thing we can do is take a detailed look at the image and find some way to differentiate the thing of interest in that image from the background. In our case our item of interest is the lane lines on the road. \n",
    "\n",
    "![alt text][original]\n",
    "\n",
    "If we look into the sample image above we can clearly identify that the lane lines are usually white or yellow in color. So the step one would be as follows.\n",
    "\n",
    "  > __ Select yellow and white color regions from the image __\n",
    "\n",
    "The second thing we find about the lane lines in our sample images is that the lane lines have a good deal of contrast difference with the background which is the road. This means that we can do edge detection in these images to find the boundaries of lane lines. Thus the step two will be as follows.\n",
    "\n",
    "  > __ Do edge detection to get the edges of lane lines __\n",
    "\n",
    "Another thing to observe is that lane lines are not everywhere in the image they are in the bottom half towards the middle. This means that we can shorten our search within a trapezoid within the image. Thus the step 3 will be:\n",
    "\n",
    "  > __ Identify a region on interest and discard other regions __\n",
    "\n",
    "With that step we are left with identifying lines in the edge detected image. This can be done by Hough lane detection algorithm. Thus the forth step will be to find straight lines in the edge detected image using hough transform.\n",
    "\n",
    "  > __Find lines in edge detected image__\n",
    "\n",
    "If you have ever worked with hough line detection, then you will know that multiple lines gets detected. We just want one line on right and one on left. So we need to group and average the lines we get out of hough line detection. To identify / group lane lines we can use the fact that slope of lines will be greater than or less than zero depending on there position on the road. \n",
    "\n",
    "  > __Group and average lines besed on slope__\n",
    "\n",
    "Finaly we can extend this pipeline for a video stream.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "1. Select yellow and white color regions from the image\n",
    "2. Do edge detection to get the edges of lane lines\n",
    "3. Identify a region on interest and discard other regions\n",
    "4. Find lines in edge detected image\n",
    "5. Group and average lines based on slope\n",
    "\n",
    "---\n",
    "\n",
    "### Steps\n",
    "\n",
    "#### 1. Color selection\n",
    "\n",
    "##### RGB color space\n",
    "We know that lane lines are white or yellow in color based on this information we are trying to extract regions with lane lines from the images. The images are in RGB color space and a particular range of colors can be extracted from the image using opencv's `cv2.inRange()` function. We pass in the image and two sets of RGB values and the function outputs the regions in that particular color range.\n",
    "\n",
    "Reference: [RGB Color Code Chart](http://www.rapidtables.com/web/color/RGB_Color.htm)\n",
    "\n",
    "```python\n",
    "def rgb_color_select(image):\n",
    "    \"\"\"\n",
    "    Seperate white and yellow color regions from\n",
    "    the given image based on RGB values\n",
    "    \"\"\"\n",
    "    # creating white mask\n",
    "    lower_range = np.uint8([180,180,180])\n",
    "    upper_range = np.uint8([255,255,255])\n",
    "    white_image = cv2.inRange(image, lower_range, upper_range)\n",
    "    \n",
    "    #creating yellow mask\n",
    "    lower_range = np.uint8([190, 190, 0])\n",
    "    upper_range = np.uint8([255,255,255])\n",
    "    yellow_image = cv2.inRange(image, lower_range, upper_range)\n",
    "    \n",
    "    #combine both the masks\n",
    "    mask = cv2.bitwise_or(white_image, yellow_image)\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    \n",
    "    return masked_image\n",
    "```\n",
    "\n",
    "![alt_text][rgb_select]\n",
    "\n",
    "This worked fine, but not for all images. The images where there are shadows, the operation did not yield good results.\n",
    "\n",
    "##### HSL Color space\n",
    "Now we can experiment with color selection operation in other color spaces. HSL is Hue Lightness Saturation color space.\n",
    "Here also we can use the `cv2.inRange()` function.\n",
    "\n",
    "```python\n",
    "def hsl_color_select(image):\n",
    "    \"\"\"\n",
    "    Seperate white and yellow color regions from\n",
    "    the given image based on HSL values\n",
    "    \"\"\"\n",
    "    img = hsl(image)\n",
    "    # creating white mask\n",
    "    lower_range = np.uint8([  0, 200,   0])\n",
    "    upper_range = np.uint8([255,255,255])\n",
    "    white_image = cv2.inRange(img, lower_range, upper_range)\n",
    "    \n",
    "    #creating yellow mask\n",
    "    lower_range = np.uint8([10, 0, 100])\n",
    "    upper_range = np.uint8([40, 255, 255])\n",
    "    yellow_image = cv2.inRange(img, lower_range, upper_range)\n",
    "    \n",
    "    #combine both the masks\n",
    "    mask = cv2.bitwise_or(white_image, yellow_image)\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    \n",
    "    return masked_image\n",
    "```\n",
    "![alt_text][hsl_select]\n",
    "\n",
    "This worked much better for yellow regions under the shades of trees. \n",
    "\n",
    "##### Combined color space\n",
    "\n",
    "From our above result its evident that color selection in RGB space was good at identifying white regions and that in HSL space is good at identifying yellow regions. So i combined the power of both and created a single function.\n",
    "\n",
    "```python\n",
    "def color_select(image):\n",
    "    # creating white mask\n",
    "    lower_range = np.uint8([180,180,180])\n",
    "    upper_range = np.uint8([255,255,255])\n",
    "    white_image = cv2.inRange(image, lower_range, upper_range)\n",
    "    \n",
    "    #creating yellow mask\n",
    "    lower_range = np.uint8([190, 190, 0])\n",
    "    upper_range = np.uint8([255,255,255])\n",
    "    yellow_image = cv2.inRange(image, lower_range, upper_range)\n",
    "    \n",
    "    #combine both the masks\n",
    "    rgb_mask = cv2.bitwise_or(white_image, yellow_image)\n",
    "    \n",
    "    hsl_img = hsl(image)\n",
    "    # creating white mask\n",
    "    lower_range = np.uint8([  0, 200,   0])\n",
    "    upper_range = np.uint8([255,255,255])\n",
    "    white_image = cv2.inRange(hsl_img, lower_range, upper_range)\n",
    "    \n",
    "    #creating yellow mask\n",
    "    lower_range = np.uint8([10, 0, 100])\n",
    "    upper_range = np.uint8([40, 255, 255])\n",
    "    yellow_image = cv2.inRange(hsl_img, lower_range, upper_range)\n",
    "    \n",
    "    #combine both the masks\n",
    "    hsl_mask = cv2.bitwise_or(white_image, yellow_image)\n",
    "    \n",
    "    mask = cv2.bitwise_or(rgb_mask, hsl_mask)\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    \n",
    "    return masked_image\n",
    "```\n",
    "![alt_text][color_select]\n",
    "\n",
    "This seems to produce very good results.\n",
    "\n",
    "#### 2. Convert to grayscale\n",
    "After color selection we are going forward with edge detection and other operations. These operations does not require color information and just a grayscale image is enough. So we convert our images into grayscale.\n",
    "\n",
    "```python\n",
    "def grayscale(img):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    (assuming your grayscaled image is called 'gray')\n",
    "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Or use BGR2GRAY if you read an image with cv2.imread()\n",
    "    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "```\n",
    "\n",
    "#### 3. Apply blur\n",
    "If we do edge detection on images with rough edge we are most likely to get a noisy image. So to get clear edges we apply a Gaussian blur to the images. We use opencv's `cv2.GaussianBlur()` function for this. This function takes an image and a kernel size. Kernal size is a possitive odd number. The greater the kernal size the more blurred the image gets. You can try different kernel sizes and find which works best for you. I am using kernel size 15. Refer the following links for more details.\n",
    "\n",
    "- [Gaussian Filter Theory](http://docs.opencv.org/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.html#gaussian-filter)\n",
    "- [cv2.GaussianBlur OpenCV API Reference](http://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=gaussianblur#gaussianblur)\n",
    "\n",
    "```python\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "```\n",
    "\n",
    "![alt_text][blur]\n",
    "\n",
    "This is how the images look after converting to grayscale and applying gaussian blur.\n",
    "\n",
    "#### 4. Edge Detection\n",
    "To find the edges we use canny edge detection algorithm which typically employs gradients in X and Y direction and then thins out the identified edge to a single pixel width. OpenCv has an inbuild function to do canny edge detection. This function takes a grayscale image, a low threshold and a high threshold. If a pixel gradient is higher than the upper threshold, the pixel is accepted as an edge and if it is less than the lower threshold, it is rejected. Refer the links below for more information.\n",
    "\n",
    "- [Canny Edge Detection Theory](http://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html)\n",
    "- [cv2.Canny OpenCV API Reference](http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html)\n",
    "\n",
    "```python\n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "```\n",
    "\n",
    "![alt_text][edge]\n",
    "This is how an edge detected images looks like.\n",
    "\n",
    "#### 5. Selecting Region of Interest\n",
    "As we have found the edges, the next step is to find the lines. But we don’t need to do the search for lines everywhere in the image we can see that the lane lines are in the middle bottom region. So we can constrain our search to that region alone. We can use opencv's `cv2.fillPoly()` function to do this. I have identified a trapezoidal shape where i want to restrict the search.\n",
    "\n",
    "- [cv2.fillPoly OpenCV API Reference](http://docs.opencv.org/modules/core/doc/drawing_functions.html#fillpoly)\n",
    "\n",
    "```python\n",
    "def select_region(image):\n",
    "    \"\"\"\n",
    "    Selects a region of interest from the given image\n",
    "    \"\"\"\n",
    "    rows, cols = image.shape[:2]\n",
    "    bottom_left  = [cols*0.1, rows*0.94]\n",
    "    top_left     = [cols*0.4, rows*0.6]\n",
    "    bottom_right = [cols*0.9, rows*0.94]\n",
    "    top_right    = [cols*0.6, rows*0.6] \n",
    "    vertices = np.array([[bottom_left, top_left, top_right, bottom_right]], dtype=np.int32)\n",
    "    return region_of_interest(image, vertices)\n",
    "```\n",
    "![alt_img][roi]\n",
    "\n",
    "#### 6. Line Detection\n",
    "After selecting the region of interest we are ready to find lines on the image. We use opencv's `cv2.HoughLinesP()` function to do that. This function has a few parameters.\n",
    "- rho: Distance resolution of the accumulator in pixels.\n",
    "- theta: Angle resolution of the accumulator in radians.\n",
    "- threshold: Accumulator threshold parameter. Only those lines are returned that get enough votes (> `threshold`).\n",
    "- minLineLength: Minimum line length. Line segments shorter than that are rejected.\n",
    "- maxLineGap: Maximum allowed gap between points on the same line to link them.\n",
    "\n",
    "This algorithm maps the result of canny edge detection to a space called hough space. In this space points will becomes lines and intersection of lines means that there is a straight line. Many lines get identified in the image. I use a pythons random function to generate random colors for each line. For more info refer\n",
    "\n",
    "- [Hough Line Transform Theory](http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.html)\n",
    "- [cv.HoughLinesP OpenCV API Reference](http://docs.opencv.org/modules/imgproc/doc/feature_detection.html?highlight=houghlinesp#houghlinesp)\n",
    "\n",
    "```python\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \"\"\"\n",
    "    `img` should be the output of a Canny transform.\n",
    "        \n",
    "    Returns an image with hough lines drawn.\n",
    "    \"\"\"\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    if lines is None:\n",
    "        print('no lines detected')\n",
    "        print(img.shape)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        return None\n",
    "    img_black = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    draw_lines(img_black, lines)\n",
    "    return img_black, lines\n",
    "```\n",
    "\n",
    "![alt_img][hough]\n",
    "\n",
    "Now i combined original image with the lines image.\n",
    "\n",
    "![alt_img][overlay]\n",
    "\n",
    "#### 6. Grouping, averaging and extrapolating lines\n",
    "Hough line detection gives you a set of lines. We need a single line of the left as well as on the right. We do this by the fact that the lines on the left will have a negative slope and those on the right have positive slope. We group lines by their slope and finally average them so get a single line on the left as well as on the right.\n",
    "\n",
    "```python\n",
    "def get_lanes(hough_lines):\n",
    "    \"\"\"\n",
    "    Given hough lines this function will identify lines on the left and right,\n",
    "    group and average them to output single line on left and right\n",
    "    \"\"\"\n",
    "    left_lines = []\n",
    "    right_lines = []\n",
    "    \n",
    "    for line in hough_lines:\n",
    "        slope = get_slope(line)\n",
    "        if slope > 0:\n",
    "            left_lines.append(line[0])\n",
    "        else:\n",
    "            right_lines.append(line[0])\n",
    "\n",
    "    line_right = np.int32(np.average(right_lines, axis=0))\n",
    "    line_left = np.int32(np.average(left_lines, axis=0))\n",
    "    return line_right, line_left\n",
    "    ```\n",
    "    \n",
    "This function will output a left and a right lane line. Now we need to extrapolate these lines to extend upto the top of the RIO from the bottom.\n",
    "\n",
    "```python\n",
    "def get_line_points(line, y1, y2):\n",
    "    \"\"\"\n",
    "    Given line and top and bottm points, this function extents that line \n",
    "    to the y1, y2 and returns this longer line\n",
    "    \"\"\"\n",
    "    if line is None:\n",
    "        return None\n",
    "    \n",
    "    slope, length, intercept = get_slope_length_and_intercept(line)\n",
    "    x1 = int((y1 - intercept)/slope)\n",
    "    x2 = int((y2 - intercept)/slope)\n",
    "    y1 = int(y1)\n",
    "    y2 = int(y2)\n",
    "    \n",
    "    return [x1, y1, x2, y2]\n",
    "```\n",
    "\n",
    "The above function takes a line, upper and lower y values and extrapolates the given line to those values. Thus we get a continous line in that limits.\n",
    "\n",
    "![alt_img][lane_drawn]\n",
    "\n",
    "** Thus we have succesfully identified lane lines on images. **\n",
    "\n",
    "### Extending to videos\n",
    "\n",
    "As we have successfully tested our pipeline on all our test images, lets extent our program to do the same with video streams. Video streams are just images shown at an interval called frame rate. Usually the frame rate is 24 frame per second. Which means at every second of the video there will be 24 images. We use Moviepy library to edit our test videos. `process_image()` in the following code is out pipeline function which does all the handwork for us. This function gets called every frame while processing.\n",
    "\n",
    "Note: The problem with simply running our pipeline every frame is that the lane lines drawn looks flickering. To avoid this flickering i have added an averaging operation with a queue implementation. I have defined a queue size of 62. We add every new lane line detected to the queue and average it. The line shown is actually the average of all those lines. This brings a stabilization to our detection process.\n",
    "\n",
    "```python\n",
    "    from collections import deque\n",
    "\n",
    "    QUEUE_SIZE=62\n",
    "\n",
    "    class LaneProcessor:\n",
    "    def __init__(self):\n",
    "        self.left_lines  = deque(maxlen=QUEUE_SIZE)\n",
    "        self.right_lines = deque(maxlen=QUEUE_SIZE)\n",
    "\n",
    "    def process_image(self, image):\n",
    "        rgb_masked = color_select(image)\n",
    "        gray = grayscale(rgb_masked)\n",
    "        blured = gaussian_blur(gray, kernel_size=15)\n",
    "        edge = canny(blured, low_threshold=50, high_threshold=150)\n",
    "        roi = select_region(edge)\n",
    "        line_img, lines = hough_lines(roi, 1, np.pi/180, 20, 20, 300)\n",
    "        \n",
    "        if lines is None:\n",
    "            return image\n",
    "        line_right, line_left = get_lanes(lines)\n",
    "\n",
    "        def mean_lines(line, lines):\n",
    "#             print(line)\n",
    "            if line is not None:\n",
    "                lines.append(line)\n",
    "\n",
    "            if len(lines)>0:\n",
    "                line = np.mean(lines, axis=0, dtype=np.int32)\n",
    "            return line\n",
    "\n",
    "        line_left  = mean_lines(line_left,  self.left_lines)\n",
    "        line_right = mean_lines(line_right, self.right_lines)\n",
    "        \n",
    "        blank_image = np.zeros_like(image)\n",
    "        \n",
    "        draw_line(blank_image, line_left)\n",
    "        draw_line(blank_image, line_right)\n",
    "        i = weighted_img(blank_image, image)\n",
    "\n",
    "        return i\n",
    "```\n",
    "The outputs are as follows\n",
    "\n",
    "- [White Lanes Video](./test_videos_output/solidWhiteRight.mp4)\n",
    "- [Yellow Lanes Video](./test_videos_output/solidYellowLeft.mp4)\n",
    "- [Dark Shades Video](./test_videos_output/challenge.mp4)\n",
    "\n",
    "### Shortcomings with your current pipeline\n",
    "\n",
    "\n",
    "It doesn’t work on sudden bends in the road as the region of interest is selected from the bottom middle.\n",
    "This pipeline does not take into consideration the curved nature of lane lines atleast in some parts of the road.\n",
    "Huge lighting variations might cause errors in detection.\n",
    "\n",
    "\n",
    "### Possible improvements to the pipeline\n",
    "\n",
    "This pipeline detects straight lines and expects the lane to be straight. One improvement might be to detect curves instead of lines and annotate the lanes as curves itself.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
